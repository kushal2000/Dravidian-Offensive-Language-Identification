{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLmodels.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMW2TgSwIpWdmaFCJKko4DT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"1mCQ746OYG08","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607868533043,"user_tz":-330,"elapsed":2038,"user":{"displayName":"Kushal Kedia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUcyZnmbbRRgeqAp1TfWBxUa_rY5eK6djHslS6Jw=s64","userId":"08490086646661819003"}},"outputId":"28336130-2168-450d-cd28-0315ca31e601"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount = True)\n","import os\n","root_path = 'gdrive/My Drive/EACL/'\n","os.chdir(root_path)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IpLrUjMUg7-2"},"source":["!pip install transformers\n","!pip install demoji\n","!pip install nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gap7vP_Pg9JC","executionInfo":{"status":"ok","timestamp":1607868534176,"user_tz":-330,"elapsed":3162,"user":{"displayName":"Kushal Kedia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUcyZnmbbRRgeqAp1TfWBxUa_rY5eK6djHslS6Jw=s64","userId":"08490086646661819003"}},"outputId":"85f66122-13db-4a0e-aaf8-7fd0a7ec877a"},"source":["import numpy as np\n","import pandas as pd\n","\n","import copy\n","import sklearn\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.metrics import f1_score \n","from tqdm import tqdm\n","import demoji\n","import nltk\n","import string\n","import pickle\n","import math\n","import numpy as np\n","import sys\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import mutual_info_classif\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.corpus import stopwords\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.metrics import f1_score\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.svm import SVC\n","from sklearn.svm import LinearSVC\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","\n","demoji.download_codes() \n","plt.rcParams['figure.figsize'] = [10, 8]\n","plt.rcParams.update({'font.size': 16})\n","RANDOM_SEED = 42\n","np.random.seed(RANDOM_SEED)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Downloading emoji data ...\n","... OK (Got response in 0.12 seconds)\n","Writing emoji data to /root/.demoji/codes.json ...\n","... OK\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WzXIp-T8lS5F"},"source":[""]},{"cell_type":"code","metadata":{"id":"z4AYZg1WlTc5","executionInfo":{"status":"ok","timestamp":1607868534177,"user_tz":-330,"elapsed":3160,"user":{"displayName":"Kushal Kedia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUcyZnmbbRRgeqAp1TfWBxUa_rY5eK6djHslS6Jw=s64","userId":"08490086646661819003"}}},"source":["class Tokenizer():\n","    def __init__(self):\n","        self.index = {}\n","        self.tf_idf_index = {}\n","        self.wordnet_lemmatizer = WordNetLemmatizer()\n","        self.stopwords = set(stopwords.words('english'))\n","\n","    def remove_punc(self, text):\n","        return ''.join([ch for ch in text if str(ch).isalpha() or ch == ' '])\n","    \n","    def remove_stop(self, text):\n","        return ' '.join([word for word in text.lower().split() if word not in self.stopwords])\n","    \n","    def get_wordnet_pos(self, word):\n","        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","        tag = nltk.pos_tag([word])[0][1][0].upper()\n","        tag_dict = {\"J\": wordnet.ADJ,\n","                    \"N\": wordnet.NOUN,\n","                    \"V\": wordnet.VERB,\n","                    \"R\": wordnet.ADV}\n","\n","        return tag_dict.get(tag, wordnet.NOUN)\n","\n","    def lemmatize(self, text):\n","        # return [self.wordnet_lemmatizer.lemmatize(w, self.get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n","        return [self.wordnet_lemmatizer.lemmatize(w) for w in nltk.word_tokenize(text)]\n","\n","    def build_index(self, article_id, tokenized):\n","        for (idx, token) in enumerate(tokenized):\n","            if token not in self.index.keys():\n","                self.index[token] = {}\n","            if article_id not in self.index[token].keys():\n","                self.index[token][article_id] = []\n","            self.index[token][article_id].append(idx+1)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ON4z107Qg_bZ","executionInfo":{"status":"ok","timestamp":1607868535928,"user_tz":-330,"elapsed":1696,"user":{"displayName":"Kushal Kedia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUcyZnmbbRRgeqAp1TfWBxUa_rY5eK6djHslS6Jw=s64","userId":"08490086646661819003"}}},"source":["class Dataset():\n","    def __init__(self, train_data, val_data, tokenizer, batch_size = 32):\n","        # self.train_data = train_data\n","        # self.val_data = val_data\n","        self.batch_size = batch_size\n","        self.tokenizer = tokenizer\n","        self.label_dict = {'Not_offensive': 0,\n","                    'Offensive_Targeted_Insult_Group': 3,\n","                    'Offensive_Targeted_Insult_Individual': 5,\n","                    'Offensive_Targeted_Insult_Other': 2,\n","                    'Offensive_Untargetede': 4,\n","                    'not-Kannada': 1}\n","        \n","        self.sentences_train = []\n","        self.sentences_test = []\n","\n","        self.y_train = []\n","        self.y_test = []\n","\n","        self.process_train(train_data)\n","        self.process_test(val_data)\n","        \n","        vectorizer = CountVectorizer()\n","        self.vec = vectorizer.fit(self.sentences_train)\n","\n","        self.X_train = self.vec.transform(self.sentences_train)\n","        self.X_test = self.vec.transform(self.sentences_test)\n","\n","    def process_train(self, data):  \n","        tokens = []\n","\n","        for article_id, line in enumerate(data):\n","            sentence = line.strip().split('\\t')\n","            label = sentence.pop()\n","            if label not in self.label_dict:\n","                self.label_dict[label] = len(self.label_dict)\n","            sentence = ' '.join(sentence)\n","            emoji_dict = demoji.findall(sentence)\n","            if len(emoji_dict): \n","                for emoji, text in emoji_dict.items():\n","                    sentence = sentence.replace(emoji, ' '+text+' ')\n","                    sentence = ' '.join(sentence.split())\n","            cleaned_text = tokenizer.remove_punc(sentence)\n","            removed_stop = tokenizer.remove_stop(cleaned_text)\n","            tokenized = tokenizer.lemmatize(removed_stop)\n","            self.sentences_train.append(' '.join(tokenized))\n","            self.y_train.append(label)\n","        \n","    def process_test(self, data):\n","        tokens = []\n","        \n","        for article_id, line in enumerate(data):\n","            sentence = line.strip().split('\\t')\n","            label = sentence.pop()\n","            if label not in self.label_dict:\n","                self.label_dict[label] = len(self.label_dict)\n","            sentence = ' '.join(sentence)\n","            emoji_dict = demoji.findall(sentence)\n","            if len(emoji_dict): \n","                for emoji, text in emoji_dict.items():\n","                    sentence = sentence.replace(emoji, ' '+text+' ')\n","                    sentence = ' '.join(sentence.split())\n","            cleaned_text = tokenizer.remove_punc(sentence)\n","            removed_stop = tokenizer.remove_stop(cleaned_text)\n","            tokenized = tokenizer.lemmatize(removed_stop)\n","            self.sentences_test.append(' '.join(tokenized))\n","            self.y_test.append(label)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkYG6mQIhANd","executionInfo":{"status":"ok","timestamp":1607868543373,"user_tz":-330,"elapsed":8645,"user":{"displayName":"Kushal Kedia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUcyZnmbbRRgeqAp1TfWBxUa_rY5eK6djHslS6Jw=s64","userId":"08490086646661819003"}}},"source":["tokenizer = Tokenizer()\n","with open('Dataset/kannada_offensive_train.csv', 'r') as f:\n","    train_data = f.readlines()\n","with open('Dataset/kannada_offensive_dev.csv', 'r') as f:\n","    val_data = f.readlines()\n","data = Dataset(train_data, val_data, tokenizer)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"UKkZphW1nqx5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607869083819,"user_tz":-330,"elapsed":531465,"user":{"displayName":"Kushal Kedia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUcyZnmbbRRgeqAp1TfWBxUa_rY5eK6djHslS6Jw=s64","userId":"08490086646661819003"}},"outputId":"dd4ac851-0faf-47c4-a062-876467670924"},"source":["mult_bayes_results = {}\n","ber_bayes_results = {}\n","\n","X_train, y_train = data.X_train, np.array(data.y_train)\n","X_test, y_test = data.X_test, np.array(data.y_test)\n","K = [1000, 5000, 10000, X_train.shape[0]]\n","\n","print(X_train.shape)\n","for k in K:\n","    X = SelectKBest(mutual_info_classif,k=k).fit(X_train,y_train)\n","    X_train_new = X.transform(X_train)\n","    X_test_new = X.transform(X_test)\n","    print(f'Running Bayes Models on k = {k}............')\n","    # best_feature_idxs = data.best_features[:k]\n","    # X_train_new = X_train\n","    # X_test_new = X_test\n","    \n","    clf = MultinomialNB()\n","    clf.fit(X_train_new, y_train)\n","    y_pred = clf.predict(X_test_new)\n","    mult_bayes_results[k] = f1_score(y_test, y_pred, average = 'weighted')\n","    \n","    clf = BernoulliNB()\n","    clf.fit(X_train_new, y_train)\n","    y_pred = clf.predict(X_test_new)\n","    ber_bayes_results[k] = f1_score(y_test, y_pred, average = 'weighted')\n","    print('Done')\n","\n","print(mult_bayes_results)\n","print(ber_bayes_results)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(6217, 14054)\n","Running Bayes Models on k = 1000............\n","Done\n","Running Bayes Models on k = 5000............\n","Done\n","Running Bayes Models on k = 10000............\n","Done\n","Running Bayes Models on k = 6217............\n","Done\n","{1000: 0.6236195286550859, 5000: 0.6292425212070245, 10000: 0.6302799592802667, 6217: 0.6271811867071789}\n","{1000: 0.6226093559820003, 5000: 0.625301026952548, 10000: 0.5256274286389662, 6217: 0.6025827195167139}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L1knV6q0r0Pz","executionInfo":{"status":"ok","timestamp":1607868553133,"user_tz":-330,"elapsed":3964,"user":{"displayName":"Kushal Kedia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUcyZnmbbRRgeqAp1TfWBxUa_rY5eK6djHslS6Jw=s64","userId":"08490086646661819003"}},"outputId":"c3655991-ccf6-4d3d-a620-7176f15d80a7"},"source":["svm_results = {}\n","\n","X_train, y_train = data.X_train, np.array(data.y_train)\n","X_test, y_test = data.X_test, np.array(data.y_test)\n","\n","alphas = [1, 0.1, 0.01, 0.001]\n","random_states = [5, 20, 40]\n","max_iters = [10, 15, 20]\n","\n","print(X_train.shape)\n","for alpha in alphas:\n","    for random_state in random_states:\n","        for max_iter in max_iters:\n","            print(f'Running SVM Model on = {alpha, random_state, max_iter}............')\n","            X_train_new = X_train\n","            X_test_new = X_test\n","\n","            clf = LinearSVC(C = alpha, random_state = random_state, max_iter = max_iter)\n","            clf.fit(X_train_new, y_train)\n","            y_pred = clf.predict(X_test_new)\n","            svm_results[(alpha, random_state, max_iter)] = f1_score(y_test, y_pred, average = 'weighted')\n","\n","            print('Done')\n","\n","print(svm_results)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(6217, 14054)\n","Running SVM Model on = (1, 5, 10)............\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Done\n","Running SVM Model on = (1, 5, 15)............\n","Done\n","Running SVM Model on = (1, 5, 20)............\n","Done\n","Running SVM Model on = (1, 20, 10)............\n","Done\n","Running SVM Model on = (1, 20, 15)............\n","Done\n","Running SVM Model on = (1, 20, 20)............\n","Done\n","Running SVM Model on = (1, 40, 10)............\n","Done\n","Running SVM Model on = (1, 40, 15)............\n","Done\n","Running SVM Model on = (1, 40, 20)............\n","Done\n","Running SVM Model on = (0.1, 5, 10)............\n","Done\n","Running SVM Model on = (0.1, 5, 15)............\n","Done\n","Running SVM Model on = (0.1, 5, 20)............\n","Done\n","Running SVM Model on = (0.1, 20, 10)............\n","Done\n","Running SVM Model on = (0.1, 20, 15)............\n","Done\n","Running SVM Model on = (0.1, 20, 20)............\n","Done\n","Running SVM Model on = (0.1, 40, 10)............\n","Done\n","Running SVM Model on = (0.1, 40, 15)............\n","Done\n","Running SVM Model on = (0.1, 40, 20)............\n","Done\n","Running SVM Model on = (0.01, 5, 10)............\n","Done\n","Running SVM Model on = (0.01, 5, 15)............\n","Done\n","Running SVM Model on = (0.01, 5, 20)............\n","Done\n","Running SVM Model on = (0.01, 20, 10)............\n","Done\n","Running SVM Model on = (0.01, 20, 15)............\n","Done\n","Running SVM Model on = (0.01, 20, 20)............\n","Done\n","Running SVM Model on = (0.01, 40, 10)............\n","Done\n","Running SVM Model on = (0.01, 40, 15)............\n","Done\n","Running SVM Model on = (0.01, 40, 20)............\n","Done\n","Running SVM Model on = (0.001, 5, 10)............\n","Done\n","Running SVM Model on = (0.001, 5, 15)............\n","Done\n","Running SVM Model on = (0.001, 5, 20)............\n","Done\n","Running SVM Model on = (0.001, 20, 10)............\n","Done\n","Running SVM Model on = (0.001, 20, 15)............\n","Done\n","Running SVM Model on = (0.001, 20, 20)............\n","Done\n","Running SVM Model on = (0.001, 40, 10)............\n","Done\n","Running SVM Model on = (0.001, 40, 15)............\n","Done\n","Running SVM Model on = (0.001, 40, 20)............\n","Done\n","{(1, 5, 10): 0.645651239485926, (1, 5, 15): 0.6529647469338621, (1, 5, 20): 0.6447305953073068, (1, 20, 10): 0.6391065175180628, (1, 20, 15): 0.6405731716132718, (1, 20, 20): 0.6493370571977939, (1, 40, 10): 0.6392542414800757, (1, 40, 15): 0.6462340382925487, (1, 40, 20): 0.647897957025481, (0.1, 5, 10): 0.6321698865231559, (0.1, 5, 15): 0.6313084881244942, (0.1, 5, 20): 0.6311634110284896, (0.1, 20, 10): 0.6321009878954694, (0.1, 20, 15): 0.6324093774517969, (0.1, 20, 20): 0.6311634110284896, (0.1, 40, 10): 0.6312387970373482, (0.1, 40, 15): 0.6314295067335188, (0.1, 40, 20): 0.6325016601555336, (0.01, 5, 10): 0.5587591421488138, (0.01, 5, 15): 0.5587591421488138, (0.01, 5, 20): 0.5587591421488138, (0.01, 20, 10): 0.5587591421488138, (0.01, 20, 15): 0.5587591421488138, (0.01, 20, 20): 0.5587591421488138, (0.01, 40, 10): 0.5606242201379481, (0.01, 40, 15): 0.5587591421488138, (0.01, 40, 20): 0.5587591421488138, (0.001, 5, 10): 0.4052399797820051, (0.001, 5, 15): 0.4052399797820051, (0.001, 5, 20): 0.4052399797820051, (0.001, 20, 10): 0.4052399797820051, (0.001, 20, 15): 0.4052399797820051, (0.001, 20, 20): 0.4052399797820051, (0.001, 40, 10): 0.4052399797820051, (0.001, 40, 15): 0.4052399797820051, (0.001, 40, 20): 0.4052399797820051}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3uQmuONvyDE","executionInfo":{"status":"ok","timestamp":1607868780142,"user_tz":-330,"elapsed":230151,"user":{"displayName":"Kushal Kedia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUcyZnmbbRRgeqAp1TfWBxUa_rY5eK6djHslS6Jw=s64","userId":"08490086646661819003"}},"outputId":"2ca1ff63-a1c1-4319-e7dd-ad9944781168"},"source":["rf_results = {}\n","\n","X_train, y_train = data.X_train, np.array(data.y_train)\n","X_test, y_test = data.X_test, np.array(data.y_test)\n","K = [100, 200, 500, 1000]\n","\n","print(X_train.shape)\n","for k in K:\n","#     X = SelectKBest(mutual_info_classif,k=k).fit(X_train,y_train)\n","#     X_train_new = X.transform(X_train)\n","#     X_test_new = X.transform(X_test)\n","    print(f'Running Bayes Models on k = {k}............')\n","#     # best_feature_idxs = data.best_features[:k]\n","    X_train_new = X_train\n","    X_test_new = X_test\n","\n","    clf = RandomForestClassifier(n_estimators = k)\n","    clf.fit(X_train_new, y_train)\n","    y_pred = clf.predict(X_test_new)\n","    rf_results[k] = f1_score(y_test, y_pred, average = 'weighted')\n","\n","    print('Done')\n","\n","print(rf_results)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(6217, 14054)\n","Running Bayes Models on k = 100............\n","Done\n","Running Bayes Models on k = 200............\n","Done\n","Running Bayes Models on k = 500............\n","Done\n","Running Bayes Models on k = 1000............\n","Done\n","{100: 0.6121756364185973, 200: 0.616240200740094, 500: 0.6138774132457739, 1000: 0.6104310734534301}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WiMP1zZrw1BC"},"source":[""],"execution_count":null,"outputs":[]}]}